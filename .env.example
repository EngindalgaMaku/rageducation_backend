# GROQ API Configuration (Primary - for fast inference)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.1-70b-versatile

# Ollama API Configuration (Fallback/Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
OLLAMA_GENERATION_MODEL=qwen2.5:14b

# GPU Optimization
OLLAMA_NUM_GPU=1
OLLAMA_GPU_LAYERS=-1
OLLAMA_LOAD_TIMEOUT=300
OLLAMA_REQUEST_TIMEOUT=120

# Performance Settings
TEMPERATURE=0.7
MAX_TOKENS=512
TOP_K=40
TOP_P=0.9
REPEAT_PENALTY=1.1

# Concurrency Settings (Optimized for 16GB Cloud Run)
MAX_CONCURRENT_REQUESTS=5
EMBEDDING_BATCH_SIZE=20

# Cache Settings
ENABLE_CACHE=true
CACHE_TTL=3600

# Model Cache Settings - OPTIMIZED FOR 16GB CLOUD RUN
MARKER_CACHE_DIR=/app/models
MARKER_MAX_MEMORY_MB=15000
MARKER_TIMEOUT_SECONDS=1800
MARKER_MAX_PAGES=500
MARKER_ENABLE_RESOURCE_MONITORING=true

# Marker PDF Optimization
MARKER_DISABLE_GEMINI=true
MARKER_USE_LOCAL_ONLY=true
MARKER_DISABLE_CLOUD_SERVICES=true
MARKER_DISABLE_ALL_LLM=true
MARKER_OCR_ONLY=true