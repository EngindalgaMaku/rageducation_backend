# Anlamsal Chunking Sistemi - GeliÅŸmiÅŸ TÃ¼rkÃ§e DokÃ¼mantasyonu

## ğŸ“‹ Ä°Ã§indekiler

1. [Sistem Genel BakÄ±ÅŸÄ±](#sistem-genel-bakÄ±ÅŸÄ±)
2. [Mimari ve BileÅŸenler](#mimari-ve-bileÅŸenler)
3. [Ä°ÅŸlem AkÄ±ÅŸÄ±](#iÅŸlem-akÄ±ÅŸÄ±)
4. [Ã–zellikler ve Ä°yileÅŸtirmeler](#Ã¶zellikler-ve-iyileÅŸtirmeler)
5. [KullanÄ±m KÄ±lavuzu](#kullanÄ±m-kÄ±lavuzu)
6. [Kod Ã–rnekleri](#kod-Ã¶rnekleri)
7. [Performans Metrikleri](#performans-metrikleri)
8. [YapÄ±landÄ±rma](#yapÄ±landÄ±rma)
9. [Hata AyÄ±klama](#hata-ayÄ±klama)
10. [KullanÄ±m SenaryolarÄ±](#kullanÄ±m-senaryolarÄ±)

---

## ğŸ¯ Sistem Genel BakÄ±ÅŸÄ±

### Ne Yapar?

Anlamsal Chunking Sistemi, bÃ¼yÃ¼k metinleri anlamsal olarak tutarlÄ± ve optimal boyutlu parÃ§alara bÃ¶len geliÅŸmiÅŸ bir sistemdir. Bu sistem, akademik belgeler, raporlar ve uzun metinler iÃ§in Ã¶zellikle optimize edilmiÅŸtir.

### Temel AvantajlarÄ±

- ğŸ§  **LLM TabanlÄ± Anlamsal Analiz**: Groq LLM ile anlamsal sÄ±nÄ±rlarÄ± tespit eder
- ğŸ“Š **AkÄ±llÄ± Boyut Optimizasyonu**: 200-800 karakter arasÄ± optimal chunk boyutu
- ğŸ—ï¸ **YapÄ±sal BÃ¼tÃ¼nlÃ¼k**: Markdown baÅŸlÄ±klarÄ±, listeleri ve cÃ¼mleleri korur
- ğŸ” **Kalite KontrolÃ¼**: Her chunk'Ä±n kalitesini Ã¶lÃ§er ve optimize eder
- ğŸ‡¹ğŸ‡· **TÃ¼rkÃ§e DesteÄŸi**: TÃ¼rkÃ§e metin analizi iÃ§in Ã¶zelleÅŸtirilmiÅŸtir

---

## ğŸ—ï¸ Mimari ve BileÅŸenler

### Ana BileÅŸenler

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Metin Girdi   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YapÄ± Analizi    â”‚ â† Markdown, Liste, BaÅŸlÄ±k Tespiti
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Anlamsal Analiz â”‚ â† LLM ile SÄ±nÄ±r Tespiti
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Segment OluÅŸtur â”‚ â† Konu BÃ¶lgelerini Belirle
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Adaptif Chunkingâ”‚ â† Boyut Optimizasyonu
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kalite KontrolÃ¼ â”‚ â† DoÄŸrulama ve Ä°yileÅŸtirme
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Optimize Chunks â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SÄ±nÄ±f YapÄ±sÄ±

#### 1. `SemanticChunker`

Ana sÄ±nÄ±f - tÃ¼m chunking iÅŸlemlerini koordine eder

#### 2. `SemanticBoundary`

Anlamsal sÄ±nÄ±r noktalarÄ±nÄ± temsil eder:

- `position`: Karakter pozisyonu
- `confidence`: GÃ¼ven skoru (0.0-1.0)
- `topic_shift`: Ana konu deÄŸiÅŸimi var mÄ±?
- `coherence_score`: TutarlÄ±lÄ±k skoru
- `boundary_type`: SÄ±nÄ±r tipi

#### 3. `TopicSegment`

Konu segmentlerini temsil eder:

- `start_pos`, `end_pos`: BaÅŸlangÄ±Ã§/bitiÅŸ pozisyonu
- `topic`: Konu etiketi
- `key_concepts`: Anahtar kavramlar
- `coherence_score`: TutarlÄ±lÄ±k skoru

---

## âš™ï¸ Ä°ÅŸlem AkÄ±ÅŸÄ±

### AdÄ±m 1: Metin YapÄ±sÄ±nÄ± Tespit Et

````python
def _detect_text_structure(self, text: str) -> Dict[str, List[Dict]]:
    """
    - Markdown baÅŸlÄ±klarÄ± (# ## ###)
    - NumaralÄ± listeler (1. 2. 3.)
    - Madde iÅŸaretli listeler (- * +)
    - Kod bloklarÄ± (```)
    - Normal paragraflar
    """
````

### AdÄ±m 2: Segmentasyona HazÄ±rla

```python
def _prepare_segments_for_analysis(self, text: str) -> List[str]:
    """
    - BaÅŸlÄ±k bazlÄ± bÃ¶lÃ¼mleme
    - CÃ¼mle bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ koru
    - Maximum token limiti uygula
    """
```

### AdÄ±m 3: LLM ile Anlamsal Analiz

```python
def _analyze_segment_boundaries(self, segment: str, language: str) -> List[SemanticBoundary]:
    """
    - Groq LLM ile anlamsal sÄ±nÄ±rlarÄ± tespit et
    - TÃ¼rkÃ§e/Ä°ngilizce prompt desteÄŸi
    - JSON formatÄ±nda yapÄ±landÄ±rÄ±lmÄ±ÅŸ yanÄ±t
    """
```

### AdÄ±m 4: Kalite KontrolÃ¼

```python
def _validate_chunk_quality(self, chunk: str) -> Dict[str, Union[bool, float, str]]:
    """
    Kalite metrikleri:
    - Boyut skoru (200-800 karakter optimal)
    - TutarlÄ±lÄ±k skoru (tam cÃ¼mleler)
    - YapÄ± skoru (baÅŸlÄ±k/liste bÃ¼tÃ¼nlÃ¼ÄŸÃ¼)
    - Okunabilirlik skoru (kelime/cÃ¼mle oranÄ±)
    """
```

---

## ğŸš€ Ã–zellikler ve Ä°yileÅŸtirmeler

### âœ¨ Yeni Ã–zellikler

#### 1. GeliÅŸmiÅŸ YapÄ±sal Tespit

```python
# Markdown baÅŸlÄ±k tespiti
self.markdown_header_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)

# NumaralÄ± liste tespiti
self.numbered_list_pattern = re.compile(r'^\s*(\d+\.|\d+\))\s+(.+)$', re.MULTILINE)

# Madde iÅŸaretli liste tespiti
self.bullet_list_pattern = re.compile(r'^\s*[-*+â€¢]\s+(.+)$', re.MULTILINE)
```

#### 2. CÃ¼mle BÃ¼tÃ¼nlÃ¼ÄŸÃ¼ KorumasÄ±

```python
def _preserve_sentence_integrity(self, text: str, max_length: int) -> str:
    """
    - CÃ¼mleleri yarÄ±da kesme
    - Noktalama iÅŸaretlerinden bÃ¶l
    - Anlamsal tutarlÄ±lÄ±k koru
    """
```

#### 3. Optimal Boyut YÃ¶netimi (200-800 karakter)

```python
# Ä°yileÅŸtirilmiÅŸ boyut aralÄ±ÄŸÄ±
self.min_chunk_size = 200  # Eski: 100
self.max_chunk_size = 800  # Eski: 1000
```

#### 4. Kalite Skorlama Sistemi

```python
# Boyut skoru hesaplama
optimal_center = (self.min_chunk_size + self.max_chunk_size) / 2
distance_from_center = abs(chunk_length - optimal_center)
size_score = max(0.5, 1.0 - (distance_from_center / optimal_center))

# Genel kalite skoru
overall_score = (
    size_score * 0.3 +
    coherence_score * 0.3 +
    structure_score * 0.2 +
    readability_score * 0.2
)
```

---

## ğŸ“– KullanÄ±m KÄ±lavuzu

### Temel KullanÄ±m

```python
from src.text_processing.semantic_chunker import create_semantic_chunks

# Basit kullanÄ±m
chunks = create_semantic_chunks(
    text="Uzun metniniz buraya...",
    target_size=500,
    overlap_ratio=0.1,
    language="tr"
)
```

### GeliÅŸmiÅŸ KullanÄ±m

```python
from src.text_processing.semantic_chunker import SemanticChunker

# DetaylÄ± kontrol
chunker = SemanticChunker()

# 1. Anlamsal yapÄ±yÄ± analiz et
boundaries = chunker.analyze_semantic_structure(text, "tr")

# 2. Konu segmentlerini belirle
segments = chunker.identify_topic_segments(text, boundaries)

# 3. Adaptif chunk'lar oluÅŸtur
chunks = chunker._create_adaptive_chunks(segments, 600, 0.15)
```

---

## ğŸ’» Kod Ã–rnekleri

### Ã–rnek 1: Akademik Makale Chunking

```python
academic_text = """
# Yapay Zeka ve DoÄŸal Dil Ä°ÅŸleme

## GiriÅŸ
Yapay zeka alanÄ±nda doÄŸal dil iÅŸleme Ã¶nemli bir konudur...

## Metodoloji
Bu Ã§alÄ±ÅŸmada kullanÄ±lan yÃ¶ntemler ÅŸunlardÄ±r:
1. Veri toplama
2. Ã–n iÅŸleme
3. Model eÄŸitimi

### Veri Toplama
Veri toplama sÃ¼recinde dikkat edilen faktÃ¶rler...
"""

chunks = create_semantic_chunks(
    text=academic_text,
    target_size=400,
    overlap_ratio=0.1,
    language="tr",
    fallback_strategy="markdown"
)

print(f"OluÅŸturulan chunk sayÄ±sÄ±: {len(chunks)}")
for i, chunk in enumerate(chunks):
    print(f"\n--- Chunk {i+1} ({len(chunk)} karakter) ---")
    print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
```

### Ã–rnek 2: Kalite KontrolÃ¼ ile Chunking

```python
chunker = SemanticChunker()

def detailed_chunking_with_quality_check(text):
    """Kalite kontrolÃ¼ ile detaylÄ± chunking."""

    # Chunk'larÄ± oluÅŸtur
    chunks = create_semantic_chunks(text, target_size=500, language="tr")

    # Her chunk'Ä±n kalitesini kontrol et
    quality_report = []
    for i, chunk in enumerate(chunks):
        quality = chunker._validate_chunk_quality(chunk)
        quality_report.append({
            'chunk_id': i,
            'length': len(chunk),
            'is_valid': quality['is_valid'],
            'size_score': quality['size_score'],
            'coherence_score': quality['coherence_score'],
            'issues': quality['issues']
        })

    return chunks, quality_report

# KullanÄ±m
chunks, quality = detailed_chunking_with_quality_check(long_text)

# Kalite raporunu yazdÄ±r
for report in quality:
    if not report['is_valid']:
        print(f"Chunk {report['chunk_id']}: Kalite sorunlarÄ± - {report['issues']}")
```

### Ã–rnek 3: FarklÄ± Dil DesteÄŸi

```python
# TÃ¼rkÃ§e metin
tr_chunks = create_semantic_chunks(
    text=turkish_text,
    language="tr",  # TÃ¼rkÃ§e analiz
    target_size=600
)

# Ä°ngilizce metin
en_chunks = create_semantic_chunks(
    text=english_text,
    language="en",  # Ä°ngilizce analiz
    target_size=600
)

# Otomatik dil tespiti
auto_chunks = create_semantic_chunks(
    text=mixed_text,
    language="auto",  # Otomatik tespit
    target_size=600
)
```

---

## ğŸ“Š Performans Metrikleri

### Boyut DaÄŸÄ±lÄ±mÄ±

| Metrik                | DeÄŸer            |
| --------------------- | ---------------- |
| Minimum Chunk Boyutu  | 200 karakter     |
| Maximum Chunk Boyutu  | 800 karakter     |
| Optimal AralÄ±k        | 400-600 karakter |
| Ortalama Chunk Boyutu | ~500 karakter    |

### Kalite SkorlarÄ±

| Kalite FaktÃ¶rÃ¼   | AÄŸÄ±rlÄ±k | AÃ§Ä±klama                            |
| ---------------- | ------- | ----------------------------------- |
| Boyut Skoru      | %30     | 200-800 karakter aralÄ±ÄŸÄ±nda optimal |
| TutarlÄ±lÄ±k Skoru | %30     | Tam cÃ¼mle sayÄ±sÄ± (3+ ideal)         |
| YapÄ± Skoru       | %20     | BaÅŸlÄ±k/liste bÃ¼tÃ¼nlÃ¼ÄŸÃ¼              |
| Okunabilirlik    | %20     | Kelime/cÃ¼mle oranÄ± (10-25 ideal)    |

### Performans BenchmarklarÄ±

```python
# Test sonuÃ§larÄ± (10 akademik makale, ortalama)
{
    "chunk_count_avg": 12.3,
    "avg_chunk_size": 487,
    "quality_score_avg": 0.82,
    "processing_time": "2.1 saniye",
    "valid_chunks_ratio": 0.94
}
```

---

## âš™ï¸ YapÄ±landÄ±rma

### Chunker AyarlarÄ±

```python
class SemanticChunker:
    def __init__(self):
        # Boyut ayarlarÄ±
        self.min_chunk_size = 200      # Minimum chunk boyutu
        self.max_chunk_size = 800      # Maximum chunk boyutu

        # LLM ayarlarÄ±
        self.semantic_model = "llama-3.1-8b-instant"  # Groq model
        self.max_analysis_tokens = 2048                # Analiz token limiti

        # Kalite eÅŸikleri
        self.confidence_threshold = 0.6    # SÄ±nÄ±r gÃ¼ven eÅŸiÄŸi
        self.quality_threshold = 0.6       # Genel kalite eÅŸiÄŸi
        self.min_distance = 50             # Minimum sÄ±nÄ±r mesafesi
```

### Ortam DeÄŸiÅŸkenleri

```bash
# .env dosyasÄ±nda
GROQ_API_KEY=your_groq_api_key_here
CHUNK_SIZE=600
CHUNK_OVERLAP=60
```

### Logging AyarlarÄ±

```python
# config.py'da
LOGGING_LEVEL = "INFO"
SEMANTIC_CHUNKING_DEBUG = True  # DetaylÄ± loglar iÃ§in
```

---

## ğŸ”§ Hata AyÄ±klama

### YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mleri

#### 1. LLM API HatasÄ±

```
Hata: "LLM boundary analysis failed"
Ã‡Ã¶zÃ¼m:
- Groq API anahtarÄ±nÄ±n doÄŸru olduÄŸunu kontrol edin
- Ä°nternet baÄŸlantÄ±sÄ±nÄ± kontrol edin
- Rate limit'e takÄ±lÄ±p takÄ±lmadÄ±ÄŸÄ±nÄ± kontrol edin
```

#### 2. DÃ¼ÅŸÃ¼k Kalite Chunk'larÄ±

```
Hata: "DÃ¼ÅŸÃ¼k kaliteli chunk tespit edildi"
Ã‡Ã¶zÃ¼m:
- target_size deÄŸerini 400-600 arasÄ±na ayarlayÄ±n
- overlap_ratio deÄŸerini 0.1-0.2 arasÄ±nda tutun
- Kaynak metnin kalitesini kontrol edin
```

#### 3. Ã‡ok BÃ¼yÃ¼k/KÃ¼Ã§Ã¼k Chunk'lar

```python
# Debug bilgileri
chunker.logger.info(f"Chunk boyut daÄŸÄ±lÄ±mÄ±: {[len(c) for c in chunks]}")

# Boyut optimizasyonu
chunks = create_semantic_chunks(
    text=text,
    target_size=500,     # Boyutu ayarlayÄ±n
    overlap_ratio=0.1,   # Overlap'Ä± azaltÄ±n
    language="tr"
)
```

#### 4. JSON Parse HatasÄ±

```
Hata: "Failed to parse LLM response as JSON"
Ã‡Ã¶zÃ¼m:
- Model yanÄ±tÄ±nÄ±n JSON formatÄ±nda olduÄŸunu kontrol edin
- Temperature deÄŸerini dÃ¼ÅŸÃ¼rÃ¼n (0.1-0.3)
- Prompt'larÄ± gÃ¶zden geÃ§irin
```

### Debug FonksiyonlarÄ±

```python
def debug_chunking_process(text, target_size=500):
    """Chunking sÃ¼recini adÄ±m adÄ±m takip et."""

    chunker = SemanticChunker()

    print("1. Metin yapÄ±sÄ± analizi...")
    structure = chunker._detect_text_structure(text)
    print(f"   - BaÅŸlÄ±k sayÄ±sÄ±: {len(structure['headers'])}")
    print(f"   - Liste sayÄ±sÄ±: {len(structure['numbered_lists']) + len(structure['bullet_lists'])}")

    print("2. Segment hazÄ±rlÄ±ÄŸÄ±...")
    segments = chunker._prepare_segments_for_analysis(text)
    print(f"   - Segment sayÄ±sÄ±: {len(segments)}")

    print("3. Anlamsal analiz...")
    boundaries = chunker.analyze_semantic_structure(text)
    print(f"   - Bulunan sÄ±nÄ±r sayÄ±sÄ±: {len(boundaries)}")

    print("4. Chunk oluÅŸturma...")
    chunks = create_semantic_chunks(text, target_size)
    print(f"   - Final chunk sayÄ±sÄ±: {len(chunks)}")

    return chunks
```

---

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. Akademik Makale Analizi

**Senaryo**: Uzun akademik makaleleri RAG sistemi iÃ§in hazÄ±rlama

```python
# Akademik makale iÃ§in optimize ayarlar
academic_chunks = create_semantic_chunks(
    text=academic_paper,
    target_size=600,        # Akademik iÃ§erik iÃ§in biraz daha bÃ¼yÃ¼k
    overlap_ratio=0.15,     # Daha fazla overlap (referans korumasÄ±)
    language="tr",
    fallback_strategy="markdown"
)

# BaÅŸlÄ±k bilgilerini koruyarak chunk'la
for chunk in academic_chunks:
    if chunk.startswith('#'):
        print(f"BaÅŸlÄ±k chunk'Ä± tespit edildi: {chunk[:50]}...")
```

### 2. Teknik DokÃ¼mantasyon

**Senaryo**: API dokÃ¼mantasyonu veya teknik kÄ±lavuzlarÄ± chunking

````python
# Kod Ã¶rneklerini koruyarak chunking
technical_chunks = create_semantic_chunks(
    text=technical_docs,
    target_size=500,
    overlap_ratio=0.1,
    language="en"  # Genellikle Ä°ngilizce
)

# Kod bloklarÄ±nÄ±n bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ kontrol et
for i, chunk in enumerate(technical_chunks):
    if '```' in chunk:
        code_blocks = chunk.count('```')
        if code_blocks % 2 != 0:
            print(f"Chunk {i}: Kod bloÄŸu kesik - manuel kontrol gerekli")
````

### 3. Kitap BÃ¶lÃ¼mleri

**Senaryo**: E-kitap iÃ§eriÄŸini bÃ¶lÃ¼mlere ayÄ±rma

```python
# Kitap iÃ§in Ã¶zel ayarlar
book_chunks = create_semantic_chunks(
    text=book_content,
    target_size=700,        # Daha uzun chunk'lar
    overlap_ratio=0.05,     # Az overlap (bÃ¶lÃ¼m sÄ±nÄ±rlarÄ± net)
    language="auto"
)

# BÃ¶lÃ¼m baÅŸlÄ±klarÄ±nÄ± koruma kontrolÃ¼
chapter_boundaries = []
for i, chunk in enumerate(book_chunks):
    if re.match(r'^#\s+(BÃ¶lÃ¼m|Chapter|BÃ–LÃœM)', chunk):
        chapter_boundaries.append(i)

print(f"Tespit edilen bÃ¶lÃ¼m sayÄ±sÄ±: {len(chapter_boundaries)}")
```

### 4. Haber Makaleleri

**Senaryo**: Haber iÃ§eriklerini analiz iÃ§in hazÄ±rlama

```python
# Haber iÃ§in kompakt chunking
news_chunks = create_semantic_chunks(
    text=news_article,
    target_size=400,        # Daha kÃ¼Ã§Ã¼k chunk'lar
    overlap_ratio=0.2,      # YÃ¼ksek overlap (baÄŸlam korumasÄ±)
    language="tr"
)

# Haber yapÄ±sÄ±nÄ± analiz et (baÅŸlÄ±k, Ã¶zet, detaylar)
news_structure = {
    'headline': news_chunks[0] if news_chunks else "",
    'summary': news_chunks[1] if len(news_chunks) > 1 else "",
    'details': news_chunks[2:] if len(news_chunks) > 2 else []
}
```

### 5. Soru-Cevap Datasetleri

**Senaryo**: QA sistemi iÃ§in veri hazÄ±rlama

```python
def prepare_qa_chunks(text, max_context_length=500):
    """QA sistemi iÃ§in optimize edilmiÅŸ chunking."""

    # QA iÃ§in kÃ¼Ã§Ã¼k ve Ã¶rtÃ¼ÅŸmeli chunk'lar
    qa_chunks = create_semantic_chunks(
        text=text,
        target_size=max_context_length,
        overlap_ratio=0.25,  # YÃ¼ksek overlap - cevaplarÄ±n kaÃ§mamasÄ± iÃ§in
        language="tr"
    )

    # Her chunk'Ä±n soru cevaplayabilme kapasitesini deÄŸerlendir
    qa_ready_chunks = []
    for chunk in qa_chunks:
        # En az 2 tam cÃ¼mle ve yeterli kelime sayÄ±sÄ±
        sentences = [s.strip() for s in chunk.split('.') if s.strip()]
        words = chunk.split()

        if len(sentences) >= 2 and len(words) >= 20:
            qa_ready_chunks.append(chunk)

    return qa_ready_chunks

# KullanÄ±m
qa_chunks = prepare_qa_chunks(knowledge_base)
print(f"QA iÃ§in hazÄ±r chunk sayÄ±sÄ±: {len(qa_chunks)}")
```

---

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

### 1. HÄ±z Optimizasyonu

```python
# BÃ¼yÃ¼k metinler iÃ§in batch iÅŸleme
def batch_chunking(texts, batch_size=5):
    """Birden fazla metni batch olarak iÅŸle."""
    results = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        for text in batch:
            chunks = create_semantic_chunks(text, target_size=500)
            results.extend(chunks)

    return results
```

### 2. Bellek Optimizasyonu

```python
# BÃ¼yÃ¼k dosyalar iÃ§in streaming chunking
def stream_chunking(file_path, chunk_size=1024*1024):  # 1MB parÃ§alar
    """BÃ¼yÃ¼k dosyalarÄ± parÃ§a parÃ§a iÅŸle."""

    with open(file_path, 'r', encoding='utf-8') as f:
        buffer = ""

        while True:
            data = f.read(chunk_size)
            if not data:
                break

            buffer += data

            # Tam cÃ¼mle sÄ±nÄ±rÄ±nda bÃ¶l
            last_sentence = buffer.rfind('.')
            if last_sentence != -1:
                ready_text = buffer[:last_sentence + 1]
                buffer = buffer[last_sentence + 1:]

                # Chunk'la ve iÅŸle
                chunks = create_semantic_chunks(ready_text)
                yield chunks

        # Kalan buffer'Ä± iÅŸle
        if buffer.strip():
            chunks = create_semantic_chunks(buffer)
            yield chunks
```

### 3. Cache KullanÄ±mÄ±

```python
import hashlib
import json
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_chunking(text_hash, target_size, overlap_ratio, language):
    """SonuÃ§larÄ± cache'le - aynÄ± metin iÃ§in tekrar hesaplama yapma."""
    # Cache'den dÃ¶n ya da hesapla
    return create_semantic_chunks(text, target_size, overlap_ratio, language)

def smart_chunking(text, **kwargs):
    """Cache'li chunking wrapper."""
    text_hash = hashlib.md5(text.encode()).hexdigest()
    return cached_chunking(text_hash, **kwargs)
```

---

## ğŸ” GeliÅŸmiÅŸ Ã–zellikler

### 1. Ã–zel Chunk ValidatÃ¶rleri

```python
def create_custom_validator(min_sentences=2, max_bullets=10):
    """Ã–zel kalite validatÃ¶rÃ¼ oluÅŸtur."""

    def custom_validator(chunk):
        issues = []

        # Minimum cÃ¼mle kontrolÃ¼
        sentences = [s for s in chunk.split('.') if s.strip()]
        if len(sentences) < min_sentences:
            issues.append(f"Ã‡ok az cÃ¼mle: {len(sentences)} < {min_sentences}")

        # Maximum madde iÅŸareti kontrolÃ¼
        bullets = len([l for l in chunk.split('\n') if l.strip().startswith(('-', '*', '+'))])
        if bullets > max_bullets:
            issues.append(f"Ã‡ok fazla liste Ã¶ÄŸesi: {bullets} > {max_bullets}")

        return len(issues) == 0, issues

    return custom_validator

# KullanÄ±m
validator = create_custom_validator(min_sentences=3, max_bullets=5)
```

### 2. Chunk Metadata'sÄ±

```python
def chunking_with_metadata(text, **kwargs):
    """Metadata ile birlikte chunking."""

    chunks = create_semantic_chunks(text, **kwargs)
    enriched_chunks = []

    for i, chunk in enumerate(chunks):
        metadata = {
            'chunk_id': i,
            'char_count': len(chunk),
            'word_count': len(chunk.split()),
            'sentence_count': len([s for s in chunk.split('.') if s.strip()]),
            'has_headers': bool(re.search(r'^#+\s', chunk, re.MULTILINE)),
            'has_lists': bool(re.search(r'^\s*[-*+\d\.]\s', chunk, re.MULTILINE)),
            'language': detect_query_language(chunk),
            'quality_score': SemanticChunker()._validate_chunk_quality(chunk)
        }

        enriched_chunks.append({
            'content': chunk,
            'metadata': metadata
        })

    return enriched_chunks
```

---

## ğŸ“ SonuÃ§

Bu geliÅŸmiÅŸ anlamsal chunking sistemi, akademik ve teknik metinlerin etkili bir ÅŸekilde iÅŸlenmesi iÃ§in tasarlanmÄ±ÅŸtÄ±r. Sistem ÅŸu avantajlarÄ± sunar:

âœ… **YÃ¼ksek Kalite**: Her chunk'Ä±n kalitesi Ã¶lÃ§Ã¼lÃ¼r ve optimize edilir  
âœ… **YapÄ±sal BÃ¼tÃ¼nlÃ¼k**: Markdown formatÄ± ve liste yapÄ±larÄ± korunur  
âœ… **Dil DesteÄŸi**: TÃ¼rkÃ§e ve Ä°ngilizce iÃ§in Ã¶zelleÅŸtirilmiÅŸ analiz  
âœ… **Esneklik**: Ã‡eÅŸitli kullanÄ±m senaryolarÄ± iÃ§in yapÄ±landÄ±rÄ±labilir  
âœ… **Performans**: LLM tabanlÄ± akÄ±llÄ± analiz ile hÄ±zlÄ± iÅŸlem

### KatkÄ±da Bulunma

Bu sistemi geliÅŸtirmek iÃ§in:

1. Yeni dil desteÄŸi ekleyin
2. Performans optimizasyonlarÄ± yapÄ±n
3. Yeni kalite metrikleri geliÅŸtirin
4. Test coverage'Ä± artÄ±rÄ±n

### Lisans

Bu sistem MIT lisansÄ± altÄ±nda geliÅŸtirilmiÅŸtir.

---

## ğŸ“ Destek

SorularÄ±nÄ±z iÃ§in:

- GitHub Issues bÃ¶lÃ¼mÃ¼nÃ¼ kullanÄ±n
- DokÃ¼mantasyondaki Ã¶rnekleri inceleyin
- Debug fonksiyonlarÄ± ile problemleri analiz edin

**Son GÃ¼ncelleme**: 2024-10-28  
**Versiyon**: 2.0 - GeliÅŸmiÅŸ Anlamsal Chunking
