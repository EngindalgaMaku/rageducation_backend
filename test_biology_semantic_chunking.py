#!/usr/bin/env python3
"""
Biology Document Semantic Chunking Test

GerÃ§ek biology document ile kritik sorun testleri:
- Kesik cÃ¼mle baÅŸlangÄ±Ã§larÄ± engelleniyor mu?
- Duplicate content temizleniyor mu?
- Topic boundaries doÄŸru tespit ediliyor mu?
- Minimal overlap Ã§alÄ±ÅŸÄ±yor mu?
"""

import sys
import os

# Proje kÃ¶k dizinini sys.path'e ekle
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from src.text_processing.semantic_chunker import SemanticChunker
    print("âœ… Semantic chunker baÅŸarÄ±yla import edildi")
except ImportError as e:
    print(f"âŒ Import hatasÄ±: {e}")
    sys.exit(1)

# GerÃ§ek biology document Ã¶rneÄŸi (sorunlu chunk'larÄ± iÃ§eren)
BIOLOGY_TEXT = """
# BÄ°YOLOJÄ° ve YAÅAM

Biyoloji, canlÄ±larÄ±n yapÄ±sÄ±nÄ±, yaÅŸam sÃ¼reÃ§lerini, Ã§evresiyle iliÅŸkilerini inceleyen bilim dalÄ±dÄ±r.

## CANLILARIN ORTAK Ã–ZELLÄ°KLERÄ°

TÃ¼m canlÄ±lar ortak bazÄ± Ã¶zellikler gÃ¶sterir. Bu Ã¶zellikler canlÄ±larÄ± cansÄ±z varlÄ±klardan ayÄ±rÄ±r.

### 1. HÃ¼cresel YapÄ±
BÃ¼tÃ¼n canlÄ±lar hÃ¼crelerden oluÅŸur. HÃ¼creler yaÅŸamÄ±n temel birimleridir. Tek hÃ¼creli organizmalar vardÄ±r. Ã‡ok hÃ¼creli organizmalar da vardÄ±r.

### 2. Metabolizma
Metabolizma, canlÄ±larÄ±n yaÅŸamÄ±nÄ± sÃ¼rdÃ¼rmek iÃ§in gerÃ§ekleÅŸtirdiÄŸi kimyasal olaylarÄ±n tÃ¼mÃ¼dÃ¼r. Bu sÃ¼reÃ§te enerji Ã¼retilir. Enerji kullanÄ±lÄ±r. Metabolizma iki ana bÃ¶lÃ¼mden oluÅŸur: katabolizma ve anabolizma.

### 3. BÃ¼yÃ¼me ve GeliÅŸme
CanlÄ±lar bÃ¼yÃ¼r ve geliÅŸir. BÃ¼yÃ¼me, canlÄ±nÄ±n boyutlarÄ±nÄ±n artmasÄ±dÄ±r. GeliÅŸme ise yapÄ±sal ve fonksiyonel deÄŸiÅŸimleri iÃ§erir.

### 4. Ã‡oÄŸalma
CanlÄ±lar Ã§oÄŸalma yoluyla neslini devam ettirir. Ä°ki Ã§oÄŸalma tÃ¼rÃ¼ vardÄ±r: eÅŸeyli ve eÅŸeysiz Ã§oÄŸalma.

## SU ve YAÅAM

Su, yaÅŸam iÃ§in vazgeÃ§ilmezdir. CanlÄ±larÄ±n bÃ¼yÃ¼k kÄ±smÄ± sudan oluÅŸur. Su, hÃ¼cre iÃ§i ve hÃ¼cre dÄ±ÅŸÄ± ortamda bulunur.

### Suyun YaÅŸamdaki RolÃ¼
Su birÃ§ok fonksiyona sahiptir:
- Ã‡Ã¶zÃ¼cÃ¼ gÃ¶revi yapar
- Kimyasal tepkimelerde katÄ±lÄ±r
- VÃ¼cut Ä±sÄ±sÄ±nÄ± dÃ¼zenler
- TaÅŸÄ±yÄ±cÄ± madde gÃ¶revini Ã¼stlenir

Su molekÃ¼lÃ¼ polar bir yapÄ±ya sahiptir. Bu Ã¶zellik suyun Ã§Ã¶zÃ¼cÃ¼ olmasÄ±nÄ± saÄŸlar.

## ENERJÄ° ve YAÅAÄ°N

CanlÄ±lar yaÅŸamlarÄ± boyunca enerjiye ihtiyaÃ§ duyar. Bu enerji gÃ¼neÅŸten gelir. Bitkiler fotosentez yapar. Hayvanlar bitkilerden beslenir.

Enerji transferi besin zinciri boyunca gerÃ§ekleÅŸir. Ãœreticiler enerjinin ilk halkasÄ±dÄ±r. TÃ¼keticiler enerjiyi aktarÄ±r.
"""

def test_semantic_chunking_improvements():
    """Semantic chunking iyileÅŸtirmelerini test et."""
    print("\nğŸ§¬ BÄ°YOLOJÄ° DOCUMENT SEMANTIC CHUNKING TEST")
    print("=" * 60)
    print("ğŸ“‹ GerÃ§ek biology document ile kritik sorun testleri")
    print("-" * 60)
    
    chunker = SemanticChunker()
    
    try:
        print(f"ğŸ“„ Test metni uzunluÄŸu: {len(BIOLOGY_TEXT)} karakter")
        print(f"ğŸ“„ Test metni satÄ±r sayÄ±sÄ±: {len(BIOLOGY_TEXT.split('\\n'))}")
        
        # Semantic chunking yap
        print("\\nğŸ”„ GeliÅŸmiÅŸ semantic chunking Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        chunks = chunker.create_semantic_chunks(
            text=BIOLOGY_TEXT,
            target_size=400,
            overlap_ratio=0.1,  # %10 overlap (sistem %5'e dÃ¼ÅŸÃ¼recek)
            language="tr"
        )
        
        print(f"\\nğŸ“Š SONUÃ‡LAR:")
        print(f"   OluÅŸturulan chunk sayÄ±sÄ±: {len(chunks)}")
        print(f"   Ortalama chunk boyutu: {sum(len(c) for c in chunks) // len(chunks) if chunks else 0} karakter")
        
        # Her chunk'Ä± analiz et
        print(f"\\nğŸ” CHUNK ANALÄ°ZÄ°:")
        
        issues_found = []
        improvements_noted = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\\n--- CHUNK {i} ({len(chunk)} karakter) ---")
            
            # Ä°lk 100 karakter gÃ¶ster
            preview = chunk[:100].replace('\\n', ' ').strip()
            print(f"ğŸ“ Ä°Ã§erik: {preview}{'...' if len(chunk) > 100 else ''}")
            
            # 1. Kesik cÃ¼mle baÅŸlangÄ±cÄ± kontrolÃ¼
            first_word = chunk.split()[0] if chunk.split() else ""
            if first_word.lower() in ['inceleyen', 'olan', 'eden', 'yapan', 'dalÄ±dÄ±r']:
                issues_found.append(f"âŒ Chunk {i}: Kesik cÃ¼mle baÅŸlangÄ±cÄ± - '{first_word}'")
            elif chunk[0].isupper():
                improvements_noted.append(f"âœ… Chunk {i}: Tam cÃ¼mle ile baÅŸlÄ±yor")
            
            # 2. BaÅŸlÄ±k ile biterek sorunlu bÃ¶lÃ¼nme kontrolÃ¼
            if "## " in chunk and chunk.strip().endswith("## "):
                issues_found.append(f"âŒ Chunk {i}: BaÅŸlÄ±k ile bitiyor, iÃ§erik kopuk")
            
            # 3. Anlamsal tutarlÄ±lÄ±k kontrolÃ¼ (basit)
            sentences = [s.strip() for s in chunk.split('.') if s.strip()]
            if len(sentences) >= 3:
                # Ã‡ok farklÄ± konular var mÄ± basit kontrol
                topics = []
                for sentence in sentences[:3]:
                    if 'hÃ¼cre' in sentence.lower():
                        topics.append('hÃ¼cre')
                    elif 'su' in sentence.lower():
                        topics.append('su')  
                    elif 'enerji' in sentence.lower():
                        topics.append('enerji')
                    elif 'metabolizma' in sentence.lower():
                        topics.append('metabolizma')
                        
                unique_topics = set(topics)
                if len(unique_topics) > 2:
                    issues_found.append(f"âŒ Chunk {i}: Ã‡ok farklÄ± konular ({', '.join(unique_topics)})")
                else:
                    improvements_noted.append(f"âœ… Chunk {i}: TutarlÄ± konu ({', '.join(unique_topics) if unique_topics else 'genel'})")
        
        # 4. Duplicate content kontrolÃ¼
        print(f"\\nğŸ” DUPLICATE CONTENT KONTROLÃœ:")
        chunk_sentences = []
        for i, chunk in enumerate(chunks):
            sentences = [s.strip().lower() for s in chunk.split('.') if s.strip()]
            chunk_sentences.extend([(i+1, s) for s in sentences])
        
        duplicates_found = []
        seen_sentences = {}
        for chunk_id, sentence in chunk_sentences:
            if len(sentence) > 20:  # Sadece uzun cÃ¼mleleri kontrol et
                if sentence in seen_sentences:
                    duplicates_found.append(f"âŒ Duplicate: Chunk {chunk_id} ve {seen_sentences[sentence]} - '{sentence[:50]}...'")
                else:
                    seen_sentences[sentence] = chunk_id
        
        if not duplicates_found:
            improvements_noted.append("âœ… Duplicate content temizlendi")
        else:
            issues_found.extend(duplicates_found)
        
        # 5. Topic boundaries kontrolÃ¼
        print(f"\\nğŸ” TOPIC BOUNDARIES KONTROLÃœ:")
        for i, chunk in enumerate(chunks, 1):
            if "##" in chunk and "###" in chunk:
                # Ana baÅŸlÄ±k ile alt baÅŸlÄ±k aynÄ± chunk'ta - iyi
                improvements_noted.append(f"âœ… Chunk {i}: BaÅŸlÄ±k ve iÃ§erik birlikte")
            elif chunk.strip().startswith("##") and len(chunk.strip()) < 50:
                # Sadece baÅŸlÄ±k olan chunk - sorunlu
                issues_found.append(f"âŒ Chunk {i}: Sadece baÅŸlÄ±k, iÃ§erik yok")
        
        # SONUÃ‡ RAPORU
        print("\\n" + "=" * 60)
        print("ğŸ“Š SEMANTIC CHUNKING Ä°YÄ°LEÅTÄ°RME TEST SONUÃ‡LARI")
        print("=" * 60)
        
        if improvements_noted:
            print("\\nâœ… Ä°YÄ°LEÅTÄ°RMELER:")
            for improvement in improvements_noted:
                print(f"   {improvement}")
        
        if issues_found:
            print("\\nâŒ KALAN SORUNLAR:")  
            for issue in issues_found:
                print(f"   {issue}")
        
        # BaÅŸarÄ± oranÄ± hesapla
        total_checks = len(improvements_noted) + len(issues_found)
        success_rate = (len(improvements_noted) / total_checks * 100) if total_checks > 0 else 0
        
        print(f"\\nğŸ“ˆ BAÅARI ORANI:")
        print(f"   Ä°yileÅŸtirme sayÄ±sÄ±: {len(improvements_noted)}")
        print(f"   Kalan sorun sayÄ±sÄ±: {len(issues_found)}")  
        print(f"   BaÅŸarÄ± oranÄ±: {success_rate:.1f}%")
        
        # SonuÃ§
        if success_rate >= 80:
            print("\\nğŸ‰ BAÅARILI! Semantic chunking kritik sorunlarÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de Ã§Ã¶zÃ¼lmÃ¼ÅŸ.")
            return True
        elif success_rate >= 60:
            print("\\nâš ï¸ ORTA! BazÄ± iyileÅŸtirmeler var ama daha fazla geliÅŸim gerekli.")
            return False
        else:
            print("\\nâŒ BAÅARISIZ! Kritik sorunlar hala mevcut.")
            return False
            
    except Exception as e:
        print(f"âŒ Test hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        success = test_semantic_chunking_improvements()
        
        print(f"\\nğŸ Biology document test tamamlandÄ±")
        print(f"ğŸ¯ SONUÃ‡: {'BAÅARILI' if success else 'GeliÅŸim gerekli'}")
        
        if success:
            print("\\nğŸ’¡ Ã–NEMLÄ° Ä°YÄ°LEÅTÄ°RMELER:")
            print("   âœ… Kesik cÃ¼mle baÅŸlangÄ±Ã§larÄ± engellendi")
            print("   âœ… Duplicate content temizlendi") 
            print("   âœ… Topic boundaries iyileÅŸtirildi")
            print("   âœ… Overlap optimize edildi")
            print("   âœ… Anlamsal bÃ¼tÃ¼nlÃ¼k korundu")
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\\nâ›” Test kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        sys.exit(1)
    except Exception as e:
        print(f"\\nğŸ’¥ Beklenmeyen hata: {e}")
        sys.exit(1)